{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"ClusterCast.settings\"\n",
    "import django\n",
    "django.setup()\n",
    "import sys\n",
    "sys.path.append(\"/home/ajp031/StockDeepLearning/ClusterCast/ClusterCast\")\n",
    "from django.db.models.functions import Now\n",
    "from asgiref.sync import sync_to_async\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, RepeatVector, TimeDistributed, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "import ClusterPipeline.models.RNNModels as rnn\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import ClusterPipeline.models.ClusterProcessing as cp\n",
    "import ClusterPipeline.models.RNNModels as rnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync_to_async\n",
    "def get_all_objects(features,steps = None):\n",
    "    # Force the query to execute and load all results into memory\n",
    "    params = list(cp.StockClusterGroupParams.objects.all())\n",
    "    matching_groups = []\n",
    "    for param in params:\n",
    "        if steps is None:\n",
    "            if param.cluster_features == features:\n",
    "                group = cp.StockClusterGroup.objects.get(pk=param.pk)\n",
    "                matching_groups.append(group)\n",
    "                group.load_saved_group()\n",
    "\n",
    "        else: \n",
    "            if param.cluster_features == features and param.n_steps == steps:\n",
    "                group = cp.StockClusterGroup.objects.get(pk=param.pk)\n",
    "                matching_groups.append(group)\n",
    "                group.load_saved_group()\n",
    "                print(group.group_params.tickers)\n",
    "    \n",
    "    return matching_groups\n",
    "\n",
    "\n",
    "cluster_features = [\"close\", \"bb_low\", \"bb_high\"]\n",
    "steps = 20\n",
    "\n",
    "async def create_cluster_group_params(cluster_features, steps=20):\n",
    "\n",
    "    cluster_groups = await get_all_objects(features=cluster_features,steps=steps)\n",
    "    return cluster_groups\n",
    "\n",
    "# Run the async function\n",
    "\n",
    "matching_groups = await create_cluster_group_params(cluster_features, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync_to_async\n",
    "def get_all_clusters(groups):\n",
    "    clusters = []\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        clusters += group.clusters\n",
    "\n",
    "    for cluster in clusters:\n",
    "        models = rnn.RNNModel.objects.filter(cluster=cluster)\n",
    "        for model in models: \n",
    "            if model:\n",
    "                features += (model.model_features)\n",
    "\n",
    "    features = list(set(features))\n",
    "    return clusters, features\n",
    "\n",
    "clusters, all_features = await get_all_clusters(matching_groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to pad a 2D array to have the target number of rows\n",
    "def pad_array(lst, target_rows):\n",
    "    # Calculate how many rows to pad\n",
    "    arr = np.array(lst)\n",
    "    padding_rows = target_rows - arr.shape[0]\n",
    "    # Apply padding only to the rows\n",
    "    return np.pad(arr, pad_width=((0, padding_rows), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "max_rows = max(len(cluster.centroid) for cluster in clusters)\n",
    "padded_centroids = [pad_array(cluster.centroid, max_rows) for cluster in clusters]\n",
    "\n",
    "# Convert the list of 2D arrays to a 3D numpy array\n",
    "centroids_3d = np.stack(padded_centroids)\n",
    "centroids_3d.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "def cluster_centroids(centroids_3d, clusters,n_clusters=10):\n",
    "    # Create the clustering model\n",
    "    cluster_alg = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=5, random_state=0)\n",
    "    # Fit the clustering model\n",
    "    labels = cluster_alg.fit_predict(centroids_3d)\n",
    "    # Get the cluster centroids\n",
    "    centroids = cluster_alg.cluster_centers_\n",
    "\n",
    "    centroid_group = {}\n",
    "    for cluster,label in zip(clusters,labels):\n",
    "        if label not in centroid_group:\n",
    "            centroid_group[label] = []\n",
    "        centroid_group[label].append(cluster)\n",
    "    \n",
    "\n",
    "    return centroid_group, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_centroid_group,centroids = cluster_centroids(centroids_3d, clusters, n_clusters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_centroid_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_label = first_centroid_group[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n random features from features list \n",
    "import random\n",
    "from scipy.interpolate import interp1d\n",
    "from copy import deepcopy\n",
    "\n",
    "def front_pad_3d_array(arr, target_cols):\n",
    "    padding_cols = target_cols - arr.shape[1]\n",
    "    pad_width = ((0, 0), (padding_cols, 0), (0, 0))  # Front padding\n",
    "    return np.pad(arr, pad_width=pad_width, mode='constant', constant_values=0.0)\n",
    "def back_pad_3d_array(arr, target_cols):\n",
    "    padding_cols = target_cols - arr.shape[1]\n",
    "    pad_width = ((0, 0), (0, padding_cols), (0, 0))  # Back padding\n",
    "    return np.pad(arr, pad_width=pad_width, mode='constant', constant_values=0.0)\n",
    "\n",
    "def resample_time_series(data, max_time_steps):\n",
    "    data = deepcopy(data)\n",
    "    n_elements, current_time_steps, n_features = data.shape\n",
    "    resampled_data = np.zeros((n_elements, max_time_steps, n_features))\n",
    "\n",
    "    for i in range(n_elements):\n",
    "        for j in range(n_features):\n",
    "            x = np.linspace(0, 1, current_time_steps)\n",
    "            x_new = np.linspace(0, 1, max_time_steps)\n",
    "            f = interp1d(x, data[i, :, j], kind='linear')\n",
    "            resampled_data[i, :, j] = f(x_new)\n",
    "\n",
    "    return resampled_data\n",
    "\n",
    "max_cols = max(cluster.X_train.shape[1] for cluster in first_label)\n",
    "# Pad each X_train and collect them in a list\n",
    "for cluster in first_label:\n",
    "    cluster.X_train_resampled = resample_time_series(cluster.X_train, max_cols)\n",
    "    cluster.X_test_resampled = resample_time_series(cluster.X_test, max_cols)\n",
    "\n",
    "\n",
    "# Concatenate the list of padded X_train arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered = []\n",
    "y_train_filtered = []\n",
    "X_test_filtered = []\n",
    "y_test_filtered = []\n",
    "features = random.sample(all_features, 75)\n",
    "for cluster in first_label:\n",
    "    feautre_dict = cluster.X_feature_dict\n",
    "    y_train = cluster.y_train\n",
    "    y_test = cluster.y_test\n",
    "    print(feautre_dict)\n",
    "    cols_indices = [feautre_dict[feature] for feature in features]\n",
    "\n",
    "    filtered_train_sampled = cluster.X_train_resampled[:,:,cols_indices]\n",
    "    filtered_test_sampled = cluster.X_test_resampled[:,:,cols_indices]\n",
    "    \n",
    "    cluster.X_train_filtered_sampled = filtered_train_sampled\n",
    "    cluster.X_test_filtered_sampled = filtered_test_sampled\n",
    "\n",
    "    cluster.X_train_filtered = cluster.X_train[:,:,cols_indices]\n",
    "    cluster.X_test_filtered = cluster.X_test[:,:,cols_indices]\n",
    "    # print(str(filtered.shape) + \" \" + str(X_train.shape))\n",
    "    X_train_filtered.append(filtered_train_sampled)\n",
    "    y_train_filtered.append(y_train)\n",
    "    X_test_filtered.append(filtered_test_sampled)\n",
    "    y_test_filtered.append(y_test)\n",
    "\n",
    "all_X_train = np.concatenate(X_train_filtered)\n",
    "all_X_train.shape\n",
    "all_y_train = np.concatenate(y_train_filtered)\n",
    "\n",
    "all_X_test = np.concatenate(X_test_filtered)\n",
    "all_y_test = np.concatenate(y_test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "first_cluster = first_label[1]\n",
    "plt.plot(first_cluster.X_train_filtered_sampled[0,:,-1])\n",
    "plt.show()\n",
    "plt.plot(first_cluster.X_train_filtered[0,:,-1])\n",
    "plt.show()\n",
    "# graph the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dropout, TimeDistributed, Dense, Concatenate, Permute, Reshape, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Activation, Flatten\n",
    "def create_model(input_shape, latent_dim=6):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(None, input_shape))\n",
    "\n",
    "    # masking_layer = Masking(mask_value=0.0, name='masking_layer')(input_layer)\n",
    "\n",
    "    # Encoder\n",
    "    encoder_lstm1 = LSTM(units=500, activation='tanh', return_sequences=True, name='encoder_lstm_1_freeze')(input_layer)\n",
    "    encoder_dropout1 = Dropout(0.2, name='encoder_dropout_1_freeze')(encoder_lstm1)\n",
    "\n",
    "    encoder_lstm2 = LSTM(units=100, activation='tanh', return_sequences=True, name='encoder_lstm_2_restore')(encoder_dropout1)\n",
    "    encoder_dropout2 = Dropout(0.2, name='encoder_dropout_2_restore')(encoder_lstm2)\n",
    "\n",
    "    encoder_lstm3 = LSTM(units=100, activation='tanh', return_sequences=True, name='encoder_lstm_3_restore')(encoder_dropout2)\n",
    "    encoder_dropout3 = Dropout(0.2, name='encoder_dropout_3_restore')(encoder_lstm3)\n",
    "\n",
    "    encoder_lstm4 = LSTM(units=50, activation='tanh', return_sequences=False, name='encoder_lstm_4_restore')(encoder_dropout3)\n",
    "    encoder_dropout4 = Dropout(0.2, name='encoder_dropout_4_restore')(encoder_lstm4)\n",
    "\n",
    "    # Repeat Vector\n",
    "    repeat_vector = RepeatVector(latent_dim, name='repeat_vector')(encoder_dropout4)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_lstm1 = LSTM(units=6, activation='tanh', return_sequences=True, name='decoder_lstm_1_restore')(repeat_vector)\n",
    "    decoder_dropout1 = Dropout(0.2, name='decoder_dropout_1_restore')(decoder_lstm1)\n",
    "\n",
    "    # decoder_lstm2 = LSTM(units=50, activation='tanh', return_sequences=True, name='decoder_lstm_2_restore')(decoder_dropout1)\n",
    "    # decoder_dropout2 = Dropout(0.2, name='decoder_dropout_2_restore')(decoder_lstm2)\n",
    "\n",
    "    # decoder_lstm3 = LSTM(units=6, activation='tanh', return_sequences=True, name='decoder_lstm_3_restore')(decoder_dropout2)\n",
    "    # decoder_dropout2 = Dropout(0.2, name='decoder_dropout_3_restore')(decoder_lstm3)\n",
    "\n",
    "    \n",
    "\n",
    "    time_distributed_output = TimeDistributed(Dense(1), name='time_distributed_output')(decoder_dropout1)\n",
    "\n",
    "    # Create the model\n",
    "    model_lstm = Model(inputs=input_layer, outputs=time_distributed_output)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model_lstm.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    return model_lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Attention Layer\n",
    "\n",
    "def create_model_with_attention2(input_shape, latent_dim=6):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(None, input_shape))\n",
    "\n",
    "    # Encoder\n",
    "\n",
    "    encoder_lstm1 = LSTM(units=50, activation='tanh', return_sequences=True, name='encoder_lstm_1_restore')(input_layer)\n",
    "    encoder_dropout1 = Dropout(0.2, name='encoder_dropout_1_restore')(encoder_lstm1)\n",
    "\n",
    "    encoder_lstm2 = LSTM(units=25, activation='tanh', return_sequences=True, name='encoder_lstm_2_restore')(encoder_dropout1)\n",
    "    encoder_dropout2 = Dropout(0.2, name='encoder_dropout_2_restore')(encoder_lstm2)\n",
    "\n",
    "    encoder_lstm3 = LSTM(units=10, activation='tanh', return_sequences=True, name='encoder_lstm_3_restore')(encoder_dropout2)\n",
    "    encoder_dropout3 = Dropout(0.2, name='encoder_dropout_3_restore')(encoder_lstm3)\n",
    "\n",
    "    output_lstm = LSTM(units=6, activation='tanh', return_sequences=True, name='outputLSTM')(encoder_dropout3)\n",
    "    encoder_output = Dropout(0.2, name='encoder_output')(output_lstm)\n",
    "\n",
    "    # encoder_lstm4 = LSTM(units=50, activation='tanh', return_sequences=True, name='encoder_lstm_4_restore')(encoder_dropout3)\n",
    "    # encoder_dropout4 = Dropout(0.2, name='encoder_dropout_4_restore')(encoder_lstm4)\n",
    "\n",
    "    # Attention Layer\n",
    "    # attention = AttentionLayer(name='attention_layer')(encoder_dropout4)\n",
    "     # Attention Mechanism\n",
    "    attention = Dense(1, activation='tanh')(encoder_output)\n",
    "    attention = Flatten()(attention)\n",
    "    attention_weights = Activation('softmax')(attention)\n",
    "    context = Multiply()([encoder_output, Permute([2, 1])(RepeatVector(6)(attention_weights))])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_lstm1 = LSTM(units=75, activation='tanh', return_sequences=True, name='decoder_lstm_1_restore')(context)\n",
    "    decoder_dropout1 = Dropout(0.2, name='decoder_dropout_1_restore')(decoder_lstm1)\n",
    "\n",
    "    decoder_lstm2 = LSTM(units=25, activation='tanh', return_sequences=True, name='decoder_lstm_2_restore')(decoder_dropout1)\n",
    "    decoder_dropout2 = Dropout(0.2, name='decoder_dropout_2_restore')(decoder_lstm2)\n",
    "\n",
    "    decoder_lstm3 = LSTM(units=10, activation='tanh', return_sequences=True, name='decoder_lstm_3_restore')(decoder_dropout2)\n",
    "    decoder_dropout3 = Dropout(0.2, name='decoder_dropout_3_restore')(decoder_lstm3)\n",
    "\n",
    "    time_distributed_output = TimeDistributed(Dense(1), name='time_distributed_output')(decoder_dropout3)\n",
    "\n",
    "    final_output = time_distributed_output[:, -6:, :]\n",
    "\n",
    "    # Create the model\n",
    "    model_lstm = Model(inputs=input_layer, outputs=final_output)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model_lstm.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    return model_lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def save_decoder_initial_weights(model):\n",
    "    initial_weights = {}\n",
    "    for layer in model.layers:\n",
    "        if 'restore' in layer.name:  # Assuming decoder layers have 'decoder' in their names\n",
    "            initial_weights[layer.name] = copy.deepcopy(layer.get_weights())\n",
    "    return initial_weights\n",
    "\n",
    "model = create_model_with_attention2(len(features))\n",
    "decoder_weights = save_decoder_initial_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1086, 20, 75)\n",
      "(1086, 6)\n"
     ]
    }
   ],
   "source": [
    "print(all_X_train.shape)\n",
    "print(all_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "model.fit(all_X_train, all_y_train,validation_data=(all_X_test,all_y_test), epochs=40, batch_size=32, verbose=1)\n",
    "for layer in model.layers:\n",
    "    if 'freeze' in layer.name:\n",
    "        print(layer.name)\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('path_to_my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "def eval_model(X_test, y_test, model,cluster):\n",
    "\n",
    "    predicted_y = model.predict(X_test)\n",
    "    predicted_y = np.squeeze(predicted_y, axis=-1)\n",
    "\n",
    "    num_days = predicted_y.shape[1]  # Assuming this is the number of days\n",
    "    results = pd.DataFrame(predicted_y, columns=[f'predicted_{i+1}' for i in range(num_days)])\n",
    "\n",
    "    for i in range(num_days):\n",
    "        results[f'real_{i+1}'] = y_test[:, i]\n",
    "\n",
    "    # Generate output string with accuracies\n",
    "    output_string = f\"Cluster Number: {cluster.label}\\n\"\n",
    "    for i in range(num_days):\n",
    "        same_day = ((results[f'predicted_{i+1}'] > 0) & (results[f'real_{i+1}'] > 0)) | \\\n",
    "                ((results[f'predicted_{i+1}'] < 0) & (results[f'real_{i+1}'] < 0))\n",
    "        accuracy = round(same_day.mean() * 100,2)\n",
    "\n",
    "        output_string += (\n",
    "            f\"Accuracy{i+1}D {accuracy}% \"\n",
    "            f\"PredictedRet: {results[f'predicted_{i+1}'].mean()} \"\n",
    "            f\"ActRet: {results[f'real_{i+1}'].mean()}\\n\"\n",
    "        )\n",
    "    \n",
    "    output_string += f\"Train set length: {len(X_train_filtered)} Test set length: {len(y_test)}\\n\"\n",
    "\n",
    "    return output_string, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cluster = first_label[2]\n",
    "X_train_cluster = first_cluster.X_train_filtered_sampled\n",
    "y_train_cluster = first_cluster.y_train\n",
    "X_test_cluster = first_cluster.X_test_filtered_sampled\n",
    "y_test_cluster = first_cluster.y_test\n",
    "print(X_train_cluster.shape)\n",
    "print(y_train_cluster.shape)\n",
    "print(X_test_cluster.shape)\n",
    "print(y_test_cluster.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('path_to_my_model.h5')\n",
    "for layer in model.layers:\n",
    "    if 'restore' in layer.name:\n",
    "        layer.set_weights(decoder_weights[layer.name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Set up early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=35, restore_best_weights=True)\n",
    "\n",
    "# Fine-tune the model using your smaller dataset\n",
    "# Assume you have 'small_train_data', 'small_train_labels', 'small_val_data', and 'small_val_labels'\n",
    "history = model.fit(X_train_cluster, y_train_cluster,\n",
    "                    validation_data=(X_test_cluster, y_test_cluster),\n",
    "                    epochs=250,\n",
    "                    batch_size=16,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance\n",
    "val_loss = model.evaluate(X_test_cluster, y_test_cluster)\n",
    "print(f'Validation loss: {val_loss}')\n",
    "fineTunedAccuracy, results_tuned = eval_model(X_test_cluster, y_test_cluster, model,first_cluster)\n",
    "del model\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchMarkModel = create_model(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "benchMarkModel.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "X_train = first_cluster.X_train_filtered\n",
    "y_train = first_cluster.y_train\n",
    "X_test = first_cluster.X_test_filtered\n",
    "y_test = first_cluster.y_test\n",
    "\n",
    "# print learning rate of benchmark model\n",
    "\n",
    "print(benchMarkModel.optimizer.lr)\n",
    "\n",
    "# Set up early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=35, restore_best_weights=True)\n",
    "\n",
    "# Fine-tune the model using your smaller dataset\n",
    "# Assume you have 'small_train_data', 'small_train_labels', 'small_val_data', and 'small_val_labels'\n",
    "history = benchMarkModel.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=250,\n",
    "                    batch_size=16,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance\n",
    "val_loss = benchMarkModel.evaluate(X_test, y_test)\n",
    "print(f'Validation loss: {val_loss}')\n",
    "\n",
    "benchMarkAccuracy,results_benchmark = eval_model(X_test, y_test, benchMarkModel,first_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "def visualize_future_distribution(results):\n",
    "    '''\n",
    "    Create stacked box and whisker plots for the predicted and real values\n",
    "    '''\n",
    "\n",
    "    fig = go.Figure()\n",
    "    print(results.shape)\n",
    "\n",
    "    for i in range(6):\n",
    "\n",
    "        fig.add_trace(go.Box(y=results[f'predicted_{i+1}'], name=f'Predicted {i}')) \n",
    "        fig.add_trace(go.Box(y=results[f'real_{i+1}'], name=f'Real {i}'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Future Performance of Cluster',\n",
    "        xaxis_title='Steps in future',\n",
    "        yaxis_title='Cumulative Percent Change'\n",
    "    ) \n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "bench_fig = visualize_future_distribution(results_benchmark)\n",
    "tuned_fig = visualize_future_distribution(results_tuned)\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "for trace in bench_fig.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "for trace in tuned_fig.data:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(benchMarkAccuracy)\n",
    "print(fineTunedAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
