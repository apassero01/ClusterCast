{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7fa4e29d5000>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/remote/apps/anaconda/2023.03-acet116/lib/python3.10/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/usr/remote/apps/anaconda/2023.03-acet116/lib/python3.10/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/usr/remote/apps/anaconda/2023.03-acet116/lib/python3.10/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/usr/remote/apps/anaconda/2023.03-acet116/lib/python3.10/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "2024-01-18 23:04:21.891609: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-18 23:04:23.177235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"ClusterCast.settings\"\n",
    "import django\n",
    "django.setup()\n",
    "import sys\n",
    "sys.path.append(\"/home/ajp031/StockDeepLearning/ClusterCast/ClusterCast\")\n",
    "from django.db.models.functions import Now\n",
    "from asgiref.sync import sync_to_async\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, RepeatVector, TimeDistributed, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "import ClusterPipeline.models.RNNModels as rnn\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import tensorflow as tf\n",
    "import ClusterPipeline.models.ClusterProcessing as cp\n",
    "import ClusterPipeline.models.RNNModels as rnn \n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and Processing Dataset\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "['pctChgclose+1', 'pctChgclose+2', 'pctChgclose+3', 'pctChgclose+4', 'pctChgclose+5', 'pctChgclose+6']\n",
      "['pctChgclose-0', 'pctChgclose-1', 'pctChgclose-2', 'pctChgclose-3', 'pctChgclose-4', 'pctChgclose-5', 'pctChgclose-6', 'pctChgclose-7', 'pctChgclose-8', 'pctChgclose-9', 'pctChgclose-10', 'pctChgclose-11', 'pctChgclose-12', 'pctChgclose-13', 'pctChgclose-14', 'pctChgclose-15']\n",
      "['pctChgclose-0', 'pctChgclose-1', 'pctChgclose-2', 'pctChgclose-3', 'pctChgclose-4', 'pctChgclose-5', 'pctChgclose-6', 'pctChgclose-7', 'pctChgclose-8', 'pctChgclose-9', 'pctChgclose-10', 'pctChgclose-11', 'pctChgclose-12', 'pctChgclose-13', 'pctChgclose-14', 'pctChgclose-15']\n",
      "            pctChgclose+1  pctChgclose+2  pctChgclose+3  pctChgclose+4  \\\n",
      "Date                                                                     \n",
      "2023-12-13       0.320937      -0.567789       0.562507       0.608088   \n",
      "2023-12-14      -0.567789       0.562507       0.608088      -1.385727   \n",
      "2023-12-15       0.562507       0.608088      -1.385727       0.948192   \n",
      "2023-12-18       0.608088      -1.385727       0.948192       0.200969   \n",
      "2023-12-19      -1.385727       0.948192       0.200969       0.422253   \n",
      "2023-12-20       0.948192       0.200969       0.422253       0.180809   \n",
      "2023-12-21       0.200969       0.422253       0.180809       0.037773   \n",
      "2023-12-22       0.422253       0.180809       0.037773      -0.289497   \n",
      "2023-12-26       0.180809       0.037773      -0.289497      -0.559636   \n",
      "2023-12-27       0.037773      -0.289497      -0.559636      -0.816669   \n",
      "2023-12-28      -0.289497      -0.559636      -0.816669      -0.322108   \n",
      "2023-12-29      -0.559636      -0.816669      -0.322108       0.136966   \n",
      "2024-01-02      -0.816669      -0.322108       0.136966       1.427593   \n",
      "2024-01-03      -0.322108       0.136966       1.427593      -0.151707   \n",
      "2024-01-04       0.136966       1.427593      -0.151707       0.565542   \n",
      "2024-01-05       1.427593      -0.151707       0.565542      -0.044064   \n",
      "2024-01-08      -0.151707       0.565542      -0.044064            NaN   \n",
      "2024-01-09       0.565542      -0.044064            NaN            NaN   \n",
      "2024-01-10      -0.044064            NaN            NaN            NaN   \n",
      "2024-01-11            NaN            NaN            NaN            NaN   \n",
      "\n",
      "            pctChgclose+5  pctChgclose+6  pctChgclose-0  pctChgclose-1  \\\n",
      "Date                                                                     \n",
      "2023-12-13      -1.385727       0.948192       1.379012       0.456723   \n",
      "2023-12-14       0.948192       0.200969       0.320937       1.379012   \n",
      "2023-12-15       0.200969       0.422253      -0.567789       0.320937   \n",
      "2023-12-18       0.422253       0.180809       0.562507      -0.567789   \n",
      "2023-12-19       0.180809       0.037773       0.608088       0.562507   \n",
      "2023-12-20       0.037773      -0.289497      -1.385727       0.608088   \n",
      "2023-12-21      -0.289497      -0.559636       0.948192      -1.385727   \n",
      "2023-12-22      -0.559636      -0.816669       0.200969       0.948192   \n",
      "2023-12-26      -0.816669      -0.322108       0.422253       0.200969   \n",
      "2023-12-27      -0.322108       0.136966       0.180809       0.422253   \n",
      "2023-12-28       0.136966       1.427593       0.037773       0.180809   \n",
      "2023-12-29       1.427593      -0.151707      -0.289497       0.037773   \n",
      "2024-01-02      -0.151707       0.565542      -0.559636      -0.289497   \n",
      "2024-01-03       0.565542      -0.044064      -0.816669      -0.559636   \n",
      "2024-01-04      -0.044064            NaN      -0.322108      -0.816669   \n",
      "2024-01-05            NaN            NaN       0.136966      -0.322108   \n",
      "2024-01-08            NaN            NaN       1.427593       0.136966   \n",
      "2024-01-09            NaN            NaN      -0.151707       1.427593   \n",
      "2024-01-10            NaN            NaN       0.565542      -0.151707   \n",
      "2024-01-11            NaN            NaN      -0.044064       0.565542   \n",
      "\n",
      "            pctChgclose-10  pctChgclose-11  ...  sumPctChgclose+14  \\\n",
      "Date                                        ...                      \n",
      "2023-12-13       -0.070342        0.099010  ...          -0.659898   \n",
      "2023-12-14        0.393746       -0.070342  ...          -0.843869   \n",
      "2023-12-15        0.591589        0.393746  ...           1.151513   \n",
      "2023-12-18       -0.524941        0.591589  ...           0.437299   \n",
      "2023-12-19       -0.019706       -0.524941  ...           0.394753   \n",
      "2023-12-20       -0.402978       -0.019706  ...           1.736416   \n",
      "2023-12-21        0.763040       -0.402978  ...                NaN   \n",
      "2023-12-22        0.429915        0.763040  ...                NaN   \n",
      "2023-12-26        0.388957        0.429915  ...                NaN   \n",
      "2023-12-27        0.456723        0.388957  ...                NaN   \n",
      "2023-12-28        1.379012        0.456723  ...                NaN   \n",
      "2023-12-29        0.320937        1.379012  ...                NaN   \n",
      "2024-01-02       -0.567789        0.320937  ...                NaN   \n",
      "2024-01-03        0.562507       -0.567789  ...                NaN   \n",
      "2024-01-04        0.608088        0.562507  ...                NaN   \n",
      "2024-01-05       -1.385727        0.608088  ...                NaN   \n",
      "2024-01-08        0.948192       -1.385727  ...                NaN   \n",
      "2024-01-09        0.200969        0.948192  ...                NaN   \n",
      "2024-01-10        0.422253        0.200969  ...                NaN   \n",
      "2024-01-11        0.180809        0.422253  ...                NaN   \n",
      "\n",
      "            sumPctChgclose+15  sumPctChgclose+2  sumPctChgclose+3  \\\n",
      "Date                                                                \n",
      "2023-12-13          -0.522932         -0.246852          0.315655   \n",
      "2023-12-14           0.583723         -0.005282          0.602806   \n",
      "2023-12-15           0.999806          1.170596         -0.215132   \n",
      "2023-12-18           1.002841         -0.777639          0.170553   \n",
      "2023-12-19           0.350689         -0.437535         -0.236566   \n",
      "2023-12-20                NaN          1.149161          1.571414   \n",
      "2023-12-21                NaN          0.623222          0.804031   \n",
      "2023-12-22                NaN          0.603061          0.640834   \n",
      "2023-12-26                NaN          0.218582         -0.070916   \n",
      "2023-12-27                NaN         -0.251724         -0.811360   \n",
      "2023-12-28                NaN         -0.849133         -1.665802   \n",
      "2023-12-29                NaN         -1.376304         -1.698412   \n",
      "2024-01-02                NaN         -1.138777         -1.001811   \n",
      "2024-01-03                NaN         -0.185142          1.242451   \n",
      "2024-01-04                NaN          1.564559          1.412852   \n",
      "2024-01-05                NaN          1.275886          1.841428   \n",
      "2024-01-08                NaN          0.413836          0.369772   \n",
      "2024-01-09                NaN          0.521478               NaN   \n",
      "2024-01-10                NaN               NaN               NaN   \n",
      "2024-01-11                NaN               NaN               NaN   \n",
      "\n",
      "            sumPctChgclose+4  sumPctChgclose+5  sumPctChgclose+6  \\\n",
      "Date                                                               \n",
      "2023-12-13          0.923743         -0.461984          0.486208   \n",
      "2023-12-14         -0.782921          0.165271          0.366240   \n",
      "2023-12-15          0.733060          0.934029          1.356282   \n",
      "2023-12-18          0.371522          0.793775          0.974584   \n",
      "2023-12-19          0.185687          0.366495          0.404268   \n",
      "2023-12-20          1.752222          1.789995          1.500498   \n",
      "2023-12-21          0.841804          0.552306         -0.007329   \n",
      "2023-12-22          0.351337         -0.208298         -1.024967   \n",
      "2023-12-26         -0.630551         -1.447220         -1.769328   \n",
      "2023-12-27         -1.628029         -1.950137         -1.813171   \n",
      "2023-12-28         -1.987910         -1.850944         -0.423351   \n",
      "2023-12-29         -1.561446         -0.133853         -0.285560   \n",
      "2024-01-02          0.425782          0.274075          0.839618   \n",
      "2024-01-03          1.090744          1.656286          1.612222   \n",
      "2024-01-04          1.978394          1.934330               NaN   \n",
      "2024-01-05          1.797364               NaN               NaN   \n",
      "2024-01-08               NaN               NaN               NaN   \n",
      "2024-01-09               NaN               NaN               NaN   \n",
      "2024-01-10               NaN               NaN               NaN   \n",
      "2024-01-11               NaN               NaN               NaN   \n",
      "\n",
      "            sumPctChgclose+7  sumPctChgclose+8  sumPctChgclose+9  \n",
      "Date                                                              \n",
      "2023-12-13          0.687177          1.109430          1.290238  \n",
      "2023-12-14          0.788493          0.969301          1.007074  \n",
      "2023-12-15          1.537091          1.574864          1.285366  \n",
      "2023-12-18          1.012357          0.722859          0.163224  \n",
      "2023-12-19          0.114771         -0.444865         -1.261534  \n",
      "2023-12-20          0.940862          0.124194         -0.197914  \n",
      "2023-12-21         -0.823998         -1.146106         -1.009140  \n",
      "2023-12-22         -1.347075         -1.210109          0.217484  \n",
      "2023-12-26         -1.632362         -0.204769         -0.356476  \n",
      "2023-12-27         -0.385578         -0.537285          0.028258  \n",
      "2023-12-28         -0.575058         -0.009515         -0.053579  \n",
      "2023-12-29          0.279982          0.235918               NaN  \n",
      "2024-01-02          0.795554               NaN               NaN  \n",
      "2024-01-03               NaN               NaN               NaN  \n",
      "2024-01-04               NaN               NaN               NaN  \n",
      "2024-01-05               NaN               NaN               NaN  \n",
      "2024-01-08               NaN               NaN               NaN  \n",
      "2024-01-09               NaN               NaN               NaN  \n",
      "2024-01-10               NaN               NaN               NaN  \n",
      "2024-01-11               NaN               NaN               NaN  \n",
      "\n",
      "[20 rows x 37 columns]\n",
      "Scaling Quant Min Max Features\n",
      "Quant Min Max Features Scaled\n",
      "Dataset Preprocessing Complete\n",
      "Creating Sequences\n",
      "{'pctChgclose+1': 0, 'pctChgclose+2': 1, 'pctChgclose+3': 2, 'pctChgclose+4': 3, 'pctChgclose+5': 4, 'pctChgclose+6': 5, 'pctChgclose-0': 6, 'pctChgclose-1': 7, 'pctChgclose-10': 8, 'pctChgclose-11': 9, 'pctChgclose-12': 10, 'pctChgclose-13': 11, 'pctChgclose-14': 12, 'pctChgclose-15': 13, 'pctChgclose-2': 14, 'pctChgclose-3': 15, 'pctChgclose-4': 16, 'pctChgclose-5': 17, 'pctChgclose-6': 18, 'pctChgclose-7': 19, 'pctChgclose-8': 20, 'pctChgclose-9': 21, 'sumPctChgclose+1': 22, 'sumPctChgclose+10': 23, 'sumPctChgclose+11': 24, 'sumPctChgclose+12': 25, 'sumPctChgclose+13': 26, 'sumPctChgclose+14': 27, 'sumPctChgclose+15': 28, 'sumPctChgclose+2': 29, 'sumPctChgclose+3': 30, 'sumPctChgclose+4': 31, 'sumPctChgclose+5': 32, 'sumPctChgclose+6': 33, 'sumPctChgclose+7': 34, 'sumPctChgclose+8': 35, 'sumPctChgclose+9': 36}\n",
      "[380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416]\n",
      "{'pctChgclose+1': 0, 'pctChgclose+2': 1, 'pctChgclose+3': 2, 'pctChgclose+4': 3, 'pctChgclose+5': 4, 'pctChgclose+6': 5, 'pctChgclose-0': 6, 'pctChgclose-1': 7, 'pctChgclose-10': 8, 'pctChgclose-11': 9, 'pctChgclose-12': 10, 'pctChgclose-13': 11, 'pctChgclose-14': 12, 'pctChgclose-15': 13, 'pctChgclose-2': 14, 'pctChgclose-3': 15, 'pctChgclose-4': 16, 'pctChgclose-5': 17, 'pctChgclose-6': 18, 'pctChgclose-7': 19, 'pctChgclose-8': 20, 'pctChgclose-9': 21, 'sumPctChgclose+1': 22, 'sumPctChgclose+10': 23, 'sumPctChgclose+11': 24, 'sumPctChgclose+12': 25, 'sumPctChgclose+13': 26, 'sumPctChgclose+14': 27, 'sumPctChgclose+15': 28, 'sumPctChgclose+2': 29, 'sumPctChgclose+3': 30, 'sumPctChgclose+4': 31, 'sumPctChgclose+5': 32, 'sumPctChgclose+6': 33, 'sumPctChgclose+7': 34, 'sumPctChgclose+8': 35, 'sumPctChgclose+9': 36}\n",
      "[380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416]\n",
      "Scaling Sequences\n",
      "Scaling Sequences Complete\n"
     ]
    }
   ],
   "source": [
    "@sync_to_async\n",
    "def get_all_objects(features,steps = None):\n",
    "    # Force the query to execute and load all results into memory\n",
    "    params = list(cp.StockClusterGroupParams.objects.all())\n",
    "    matching_groups = []\n",
    "    for param in params:\n",
    "        continue\n",
    "        if steps is None:\n",
    "            if param.cluster_features == features:\n",
    "                group = cp.StockClusterGroup.objects.get(pk=param.pk)\n",
    "                matching_groups.append(group)\n",
    "                group.load_saved_group()\n",
    "\n",
    "        else: \n",
    "            if param.cluster_features == features and param.n_steps == steps:\n",
    "                group = cp.StockClusterGroup.objects.get(pk=param.pk)\n",
    "                matching_groups.append(group)\n",
    "                group.load_saved_group()\n",
    "                print(group.group_params.tickers)\n",
    "\n",
    "    group = cp.StockClusterGroup.objects.get(pk=367)\n",
    "    group.load_saved_group()\n",
    "    \n",
    "    return matching_groups,group\n",
    "\n",
    "\n",
    "cluster_features = [\"close\", \"bb_low\", \"bb_high\"]\n",
    "steps = 20\n",
    "\n",
    "async def create_cluster_group_params(cluster_features, steps=20):\n",
    "\n",
    "    cluster_groups,group = await get_all_objects(features=cluster_features,steps=steps)\n",
    "    return cluster_groups,group\n",
    "\n",
    "# Run the async function\n",
    "\n",
    "matching_groups,group = await create_cluster_group_params(cluster_features, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync_to_async\n",
    "def get_all_clusters(groups):\n",
    "    clusters = []\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        clusters += group.clusters\n",
    "\n",
    "    for cluster in clusters:\n",
    "        models = rnn.RNNModel.objects.filter(cluster=cluster)\n",
    "        for model in models: \n",
    "            if model:\n",
    "                features += (model.model_features)\n",
    "\n",
    "    features = list(set(features))\n",
    "    return clusters, features\n",
    "\n",
    "clusters, all_features = await get_all_clusters(matching_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group = group\n",
    "print(test_group.id)\n",
    "cluster_features = test_group.group_params.cluster_features\n",
    "feature_dict = test_group.group_params.X_feature_dict\n",
    "y_feature_dict = test_group.group_params.y_feature_dict\n",
    "print(len(test_group.clusters))\n",
    "training_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_top, y_train_top, X_test_top, y_test_top = test_group.get_3d_array()\n",
    "y_train_top_reversed = y_train_top[:, ::-1]\n",
    "y_test_top_reversed = y_test_top[:, ::-1]\n",
    "training_dict[\"first\"] = (X_train_top, y_train_top, X_test_top, y_test_top)\n",
    "print(X_train_top.shape)\n",
    "print(y_train_top.shape)\n",
    "print(X_test_top.shape)\n",
    "print(y_test_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_cluster_centroid(centroid, cluster_features):\n",
    "\n",
    "    # Extract the number of features\n",
    "    num_features = centroid.shape[1]\n",
    "\n",
    "    # Set up the time steps\n",
    "    time_steps = range(centroid.shape[0])\n",
    "\n",
    "    # Plot the centroid for each feature\n",
    "    for i in range(num_features):\n",
    "        plt.scatter(time_steps, centroid[:,i], label=f'Feature {cluster_features[i]}')\n",
    "        plt.plot(time_steps, centroid[:,i], 'kx-')  # 'kx-' for black crosses with lines\n",
    "\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Feature Values')\n",
    "    plt.title('Centroid Visualization Over Time Steps')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_alg = TimeSeriesKMeans(n_clusters=2, metric=\"euclidean\", max_iter=5, random_state=0)\n",
    "X_train_top, y_train_top, X_test_top, y_test_top = training_dict[\"first\"]\n",
    "X_train_cluster = test_group.filter_by_features(X_train_top, cluster_features, feature_dict)\n",
    "X_test_cluster = test_group.filter_by_features(X_test_top, cluster_features, feature_dict)\n",
    "\n",
    "print(X_train_cluster.shape)\n",
    "print(X_test_cluster.shape)\n",
    "\n",
    "first_alg.fit(X_train_cluster)\n",
    "\n",
    "print(first_alg.cluster_centers_.shape)\n",
    "\n",
    "labels = first_alg.labels_\n",
    "target_label = 1\n",
    "\n",
    "\n",
    "visualize_cluster_centroid(first_alg.cluster_centers_[target_label], cluster_features)\n",
    "visualize_cluster_centroid(first_alg.cluster_centers_[0], cluster_features)\n",
    "\n",
    "\n",
    "X_train_top = X_train_top[labels == target_label]\n",
    "\n",
    "y_train_top = y_train_top[labels == target_label]\n",
    "\n",
    "test_labels = first_alg.predict(X_test_cluster)\n",
    "X_test_top = X_test_top[test_labels == target_label]\n",
    "y_test_top = y_test_top[test_labels == target_label]\n",
    "\n",
    "training_dict[\"second\"] = (deepcopy(X_train_top), deepcopy(y_train_top), deepcopy(X_test_top), deepcopy(y_test_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_top.shape)\n",
    "print(y_train_top.shape)\n",
    "print(X_test_top.shape)\n",
    "print(y_test_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_alg = TimeSeriesKMeans(n_clusters=2, metric=\"euclidean\", max_iter=5, random_state=0)\n",
    "X_train_top, y_train_top, X_test_top, y_test_top = training_dict[\"second\"]\n",
    "X_train_cluster = test_group.filter_by_features(X_train_top, cluster_features, feature_dict)\n",
    "X_test_cluster = test_group.filter_by_features(X_test_top, cluster_features, feature_dict)\n",
    "\n",
    "print(X_train_cluster.shape)\n",
    "print(X_test_cluster.shape)\n",
    "\n",
    "second_alg.fit(X_train_cluster)\n",
    "\n",
    "labels = second_alg.labels_\n",
    "target_label = 1\n",
    "\n",
    "visualize_cluster_centroid(second_alg.cluster_centers_[target_label], cluster_features)\n",
    "visualize_cluster_centroid(second_alg.cluster_centers_[0], cluster_features)\n",
    "\n",
    "print(set(labels))\n",
    "X_train_top = X_train_top[labels == target_label]\n",
    "\n",
    "y_train_top = y_train_top[labels == target_label]\n",
    "\n",
    "test_labels = second_alg.predict(X_test_cluster)\n",
    "X_test_top = X_test_top[test_labels == target_label]\n",
    "y_test_top = y_test_top[test_labels == target_label]\n",
    "\n",
    "training_dict[\"third\"] = (deepcopy(X_train_top), deepcopy(y_train_top), deepcopy(X_test_top), deepcopy(y_test_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_top.shape)\n",
    "print(y_train_top.shape)\n",
    "print(X_test_top.shape)\n",
    "print(y_test_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_alg = TimeSeriesKMeans(n_clusters=3, metric=\"euclidean\", max_iter=5, random_state=0)\n",
    "X_train_top, y_train_top, X_test_top, y_test_top = training_dict[\"third\"]\n",
    "X_train_cluster = test_group.filter_by_features(X_train_top, cluster_features, feature_dict)\n",
    "X_test_cluster = test_group.filter_by_features(X_test_top, cluster_features, feature_dict)\n",
    "\n",
    "print(X_train_cluster.shape)\n",
    "print(X_test_cluster.shape)\n",
    "\n",
    "third_alg.fit(X_train_cluster)\n",
    "\n",
    "labels = third_alg.labels_\n",
    "target_label = 0\n",
    "\n",
    "for i in range(3):\n",
    "    visualize_cluster_centroid(third_alg.cluster_centers_[i], cluster_features)\n",
    "\n",
    "\n",
    "print(set(labels))\n",
    "X_train_top = X_train_top[labels == target_label]\n",
    "\n",
    "y_train_top = y_train_top[labels == target_label]\n",
    "\n",
    "test_labels = third_alg.predict(X_test_cluster)\n",
    "X_test_top = X_test_top[test_labels == target_label]\n",
    "y_test_top = y_test_top[test_labels == target_label]\n",
    "\n",
    "training_dict[\"last\"] = (deepcopy(X_train_top), deepcopy(y_train_top), deepcopy(X_test_top), deepcopy(y_test_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_top.shape)\n",
    "print(y_train_top.shape)\n",
    "print(X_test_top.shape)\n",
    "print(y_test_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dropout, TimeDistributed, Dense, Concatenate, Permute, Reshape, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Activation, Flatten\n",
    "from tensorflow.keras.regularizers import L1, L2, L1L2\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def create_model(input_shape, latent_dim=6):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(None, input_shape))\n",
    "\n",
    "    # masking_layer = Masking(mask_value=0.0, name='masking_layer')(input_layer)\n",
    "\n",
    "    # Encoder\n",
    "\n",
    "    encoder_lstm2 = LSTM(units=100, activation='tanh', return_sequences=True, name='encoder_lstm_2_restore')(input_layer)\n",
    "    encoder_dropout2 = Dropout(0.2, name='encoder_dropout_2_restore')(encoder_lstm2)\n",
    "\n",
    "    encoder_lstm3 = LSTM(units=50, activation='tanh', return_sequences=False, name='encoder_lstm_3_restore')(encoder_dropout2)\n",
    "    encoder_dropout3 = Dropout(0.2, name='encoder_dropout_3_restore')(encoder_lstm3)\n",
    "\n",
    "    # encoder_lstm4 = LSTM(units=50, activation='tanh', return_sequences=False, name='encoder_lstm_4_restore')(encoder_dropout3)\n",
    "    # encoder_dropout4 = Dropout(0.2, name='encoder_dropout_4_restore')(encoder_lstm4)\n",
    "\n",
    "    # Repeat Vector\n",
    "    repeat_vector = RepeatVector(latent_dim, name='repeat_vector')(encoder_dropout3)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_lstm1 = LSTM(units=100, activation='tanh', return_sequences=True, name='decoder_lstm_1_restore')(repeat_vector)\n",
    "    decoder_dropout1 = Dropout(0.2, name='decoder_dropout_1_restore')(decoder_lstm1)\n",
    "\n",
    "    decoder_lstm2 = LSTM(units=50, activation='tanh', return_sequences=True, name='decoder_lstm_2_restore')(decoder_dropout1)\n",
    "    decoder_dropout2 = Dropout(0.2, name='decoder_dropout_2_restore')(decoder_lstm2)\n",
    "\n",
    "    time_distributed_output = TimeDistributed(Dense(1), name='time_distributed_output')(decoder_dropout2)\n",
    "\n",
    "\n",
    "    # Create the model\n",
    "    model_lstm = Model(inputs=input_layer, outputs=time_distributed_output)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model_lstm.compile(optimizer=optimizer, loss=\"mae\")\n",
    "\n",
    "    return model_lstm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modelAE(input_shape, latent_dim=6):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(None, input_shape),name = 'input_layer')\n",
    "\n",
    "    # masking_layer = Masking(mask_value=0.0, name='masking_layer')(input_layer)\n",
    "\n",
    "    # Encoder\n",
    "    encoder_lstm1 = LSTM(units=300, activation='tanh', return_state=True,return_sequences=True,\n",
    "                     name='encoder_lstm_1_freeze', kernel_regularizer=L2(.001), recurrent_regularizer=L2(.001))\n",
    "    encoder_outputs1 = encoder_lstm1(input_layer)\n",
    "    encoder_states1 = encoder_outputs1[1:]\n",
    "\n",
    "    encoder_lstm2 = LSTM(units=200, activation='tanh', return_state=True,return_sequences=True, name = 'encoder_lstm_2_freeze',\n",
    "                         )\n",
    "    encoder_outputs2 = encoder_lstm2(encoder_outputs1[0])\n",
    "    encoder_states2 = encoder_outputs2[1:]\n",
    "\n",
    "    encoder_lstm3 = LSTM(units=150, activation='tanh', return_state=True,return_sequences=True, name='encoder_lstm_3_freeze')\n",
    "    encoder_outputs3 = encoder_lstm3(encoder_outputs2[0])\n",
    "    encoder_states3 = encoder_outputs3[1:]\n",
    "\n",
    "    encoder_lstm4 = LSTM(units=100, activation='tanh', return_state=True,return_sequences=True, name='encoder_lstm_4_restore')\n",
    "    encoder_outputs4 = encoder_lstm4(encoder_outputs3[0])\n",
    "    encoder_states4 = encoder_outputs4[1:]\n",
    "\n",
    "\n",
    "    # attention = Dense(1, activation='tanh')(encoder_lstm4)\n",
    "    # attention = Flatten()(attention)\n",
    "    # attention_weights = Activation('softmax')(attention)\n",
    "    # context = Multiply()([encoder_lstm4, Permute([2, 1])(RepeatVector(6)(attention_weights))])\n",
    "\n",
    "    decoder_inputs = RepeatVector(21, name='repeat_vector')(encoder_states4[0])\n",
    "    \n",
    "\n",
    "    # Decoder\n",
    "    decoder_lstm1 = LSTM(units=300, activation='tanh', return_sequences=True, name='decoder_lstm_1_freeze',\n",
    "    \n",
    "                        )(decoder_inputs, initial_state=encoder_states1)\n",
    "    decoder_lstm2 = LSTM(units=200, activation='tanh', return_sequences=True, name='decoder_lstm_2_freeze',\n",
    "                        )(decoder_lstm1, initial_state=encoder_states2)\n",
    "    decoder_lstm3 = LSTM(units=150, activation='tanh', return_sequences=True, name='decoder_lstm_3_freeze',\n",
    "                         )(decoder_lstm2, initial_state=encoder_states3)\n",
    "    decoder_lstm4 = LSTM(units=100, activation='tanh', return_sequences=True, name='decoder_lstm_4_restore',\n",
    "                         )(decoder_lstm3, initial_state=encoder_states4)\n",
    "\n",
    "    # decoder_lstm3 = LSTM(units=5, activation='tanh', return_sequences=True, name='decoder_lstm_3_restore',\n",
    "    #                      )(decoder_dropout2)\n",
    "    # decoder_dropout3 = Dropout(0.2, name='decoder_dropout_3_restore')(decoder_lstm3)\n",
    "\n",
    "    \n",
    "\n",
    "    time_distributed_output = TimeDistributed(Dense(1), name='time_distributed_output')(decoder_lstm4)\n",
    "\n",
    "    # final_output = time_distributed_output[:, -6:, :]\n",
    "\n",
    "    # Create the model\n",
    "    model_lstm = Model(inputs=input_layer, outputs=time_distributed_output)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model_lstm.compile(optimizer=optimizer, loss=\"mae\")\n",
    "\n",
    "    return model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Attention Layer\n",
    "\n",
    "def create_model_with_attention2(input_shape, latent_dim=6):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(None, input_shape),name = 'input_layer')\n",
    "\n",
    "    # Encoder\n",
    "\n",
    "    encoder_lstm1 = LSTM(units=40, activation='tanh', return_sequences=True, name='encoder_lstm_1_freeze',kernel_regularizer=L2(.0001), recurrent_regularizer=L2(.0001))(input_layer)\n",
    "    encoder_dropout1 = Dropout(0.2, name='encoder_dropout_1_freeze')(encoder_lstm1)\n",
    "\n",
    "    encoder_lstm2 = LSTM(units=50, activation='tanh', return_sequences=True, name='encoder_lstm_2_freeze')(encoder_dropout1)\n",
    "    encoder_dropout2 = Dropout(0.2, name='encoder_dropout_2_freeze')(encoder_lstm2)\n",
    "\n",
    "    encoder_lstm3 = LSTM(units=25, activation='tanh', return_sequences=True, name='encoder_lstm_3_freeze')(encoder_dropout2)\n",
    "    encoder_dropout3 = Dropout(0.2, name='encoder_dropout_3_freeze')(encoder_lstm3)\n",
    "\n",
    "    output_lstm = LSTM(units=6, activation='tanh', return_sequences=True, name='outputLSTM_freeze')(encoder_dropout3)\n",
    "    encoder_output = Dropout(0.2, name='encoder_output_freeze')(output_lstm)\n",
    "\n",
    "    # encoder_lstm4 = LSTM(units=50, activation='tanh', return_sequences=True, name='encoder_lstm_4_restore')(encoder_dropout3)\n",
    "    # encoder_dropout4 = Dropout(0.2, name='encoder_dropout_4_restore')(encoder_lstm4)\n",
    "\n",
    "    # Attention Layer\n",
    "    # attention = AttentionLayer(name='attention_layer')(encoder_dropout4)\n",
    "     # Attention Mechanism\n",
    "    attention = Dense(1, activation='tanh')(encoder_output)\n",
    "    attention = Flatten()(attention)\n",
    "    attention_weights = Activation('softmax')(attention)\n",
    "    context = Multiply()([encoder_output, Permute([2, 1])(RepeatVector(6)(attention_weights))])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_lstm1 = LSTM(units=50, activation='tanh', return_sequences=True, name='decoder_lstm_1')(context)\n",
    "    decoder_dropout1 = Dropout(0.2, name='decoder_dropout_1')(decoder_lstm1)\n",
    "\n",
    "    decoder_lstm2 = LSTM(units=25, activation='tanh', return_sequences=True, name='decoder_lstm_2')(decoder_dropout1)\n",
    "    decoder_dropout2 = Dropout(0.2, name='decoder_dropout_2')(decoder_lstm2)\n",
    "\n",
    "    # decoder_lstm3 = LSTM(units=5, activation='tanh', return_sequences=True, name='decoder_lstm_3_restore')(decoder_dropout2)\n",
    "    # decoder_dropout3 = Dropout(0.2, name='decoder_dropout_3_restore')(decoder_lstm3)\n",
    "\n",
    "    time_distributed_output = TimeDistributed(Dense(1), name='time_distributed_output')(decoder_dropout2)\n",
    "\n",
    "    # final_output = time_distributed_output[:, -6:, :]\n",
    "\n",
    "    # Create the model\n",
    "    model_lstm = Model(inputs=input_layer, outputs=final_output)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model_lstm.compile(optimizer=optimizer, loss=\"mae\")\n",
    "\n",
    "    return model_lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mechanism(encoder_outputs, decoder_state):\n",
    "    # Assuming encoder_outputs is [batch_size, input_steps, features]\n",
    "    # and decoder_state is [batch_size, features]\n",
    "    score = Dense(encoder_outputs.shape[2])(decoder_state)  # Project decoder state\n",
    "    score = tf.expand_dims(score, 1)  # Expand dims to add input_steps axis\n",
    "    score = score + encoder_outputs  # Add to encoder outputs\n",
    "    attention_weights = Activation('softmax')(score)  # Compute attention weights\n",
    "    context_vector = tf.reduce_sum(attention_weights * encoder_outputs, axis=1)\n",
    "    return context_vector\n",
    "\n",
    "def create_autoencoder(input_steps, output_steps, features):\n",
    "    # Encoder\n",
    "\n",
    "    encoder_inputs = Input(shape=(input_steps, features))\n",
    "\n",
    "    encoder_lstm1 = LSTM(200, return_sequences=True, kernel_regularizer=L2(.001), recurrent_regularizer=L2(.001))\n",
    "    encoder_output1 = encoder_lstm1(encoder_inputs)\n",
    "\n",
    "    encoder_lstm_final = LSTM(100, return_state=True, return_sequences=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm_final(encoder_output1)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_initial_input = RepeatVector(output_steps)(state_h)  # Prepare decoder inputs\n",
    "\n",
    "\n",
    "    decoder_lstm = LSTM(100, return_sequences=True)\n",
    "    decoder_output1 = decoder_lstm(decoder_initial_input, initial_state=[state_h, state_c])\n",
    "\n",
    "    # Manually apply attention mechanism for each timestep\n",
    "    context_vectors_list1 = []\n",
    "    for t in range(output_steps):\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        context_vector_t1 = attention_mechanism(encoder_outputs, decoder_output1[:, t, :])\n",
    "        context_vectors_list1.append(context_vector_t1)\n",
    "\n",
    "    # Concatenate the list of context vectors\n",
    "    context_vectors = tf.stack(context_vectors_list1, axis=1)\n",
    "\n",
    "    # Concatenate context vectors with decoder outputs\n",
    "    decoder_combined_context1 = Concatenate(axis=-1)([context_vectors, decoder_output1])\n",
    "\n",
    "    decoder_lstm2 = LSTM(50, return_sequences=True)\n",
    "    decoder_output2 = decoder_lstm2(decoder_combined_context1)\n",
    "\n",
    "    # Manually apply attention mechanism for each timestep\n",
    "    context_vectors_list2 = []\n",
    "    for t in range(output_steps):\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        context_vector_t2 = attention_mechanism(encoder_outputs, decoder_output2[:, t, :])\n",
    "        context_vectors_list2.append(context_vector_t2)\n",
    "    \n",
    "    # Concatenate the list of context vectors\n",
    "    context_vectors2 = tf.stack(context_vectors_list2, axis=1)\n",
    "    decoder_combined_context2 = Concatenate(axis=-1)([context_vectors2, decoder_output2])\n",
    "\n",
    "\n",
    "\n",
    "    # Output layer for reconstruction\n",
    "    output = TimeDistributed(Dense(1))(decoder_combined_context2)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=encoder_inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')  # Use appropriate loss\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def custom_loss_function(y_true, y_pred, past_steps, future_weight):\n",
    "    \"\"\"\n",
    "    Custom loss function that assigns different weights to the errors in predicting \n",
    "    past and future values in a sequence.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tensor): The true values.\n",
    "    y_pred (tensor): The predicted values from the model.\n",
    "    past_steps (int): The number of steps in the sequence corresponding to past values.\n",
    "    future_steps (int): The number of steps in the sequence corresponding to future values.\n",
    "    future_weight (float): The weight to assign to the errors in the future values.\n",
    "\n",
    "    Returns:\n",
    "    tensor: The computed weighted loss.\n",
    "    \"\"\"\n",
    "    # Split the true and predicted values into past and future parts\n",
    "    y_true_past, y_true_future = y_true[:, :past_steps], y_true[:, past_steps:]\n",
    "    y_pred_past, y_pred_future = y_pred[:, :past_steps], y_pred[:, past_steps:]\n",
    "\n",
    "    # Calculate mean absolute error for past and future parts\n",
    "    past_loss = tf.keras.losses.mean_squared_error(y_true_past, y_pred_past)\n",
    "    future_loss = tf.keras.losses.mean_squared_error(y_true_future, y_pred_future)\n",
    "\n",
    "    if future_weight == 1:\n",
    "        # If future_weight is 1, calculate normal MAE across the entire sequence\n",
    "        total_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    else:\n",
    "        # Weight the future loss and combine it with the past loss\n",
    "        weighted_future_loss = future_loss * future_weight\n",
    "        total_loss = tf.reduce_mean(past_loss + weighted_future_loss)\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_features(sequence, features,X_feature_dict):\n",
    "    sequence = deepcopy(sequence)\n",
    "    feature_indices = [X_feature_dict[feature] for feature in features]\n",
    "    return sequence[:,:,feature_indices]\n",
    "\n",
    "def filter_y_by_features(sequence, features,y_feature_dict):\n",
    "    sequence = deepcopy(sequence)\n",
    "    feature_indices = [y_feature_dict[feature] for feature in features]\n",
    "    print(feature_indices)\n",
    "    return sequence[:,feature_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "# features = random.sample(all_features, 30)\n",
    "# features = [\n",
    "#     \"sumpctChg+pctChgsma30Close_4\", \"sumpctChg+pctChgema10Close_5\", \"pctChg+pctChgsma5Close\", \n",
    "#     \"sumpctChg+sma10Close_6\", \"sumpctChg+pctChgsma50Close_5\", \"sumpctChg+bb_maClose_6\", \n",
    "#     \"sumpctChglow_1\", \"sumpctChg+ema100Close_5\", \"sumpctChg+ema10Close_3\", \"sumpctChg+pctChgsma5Close_2\", \n",
    "#     \"pctChglow\", \"sumpctChgvolume_3\", \"sumpctChgema100_3\", \"sumpctChgsma10_4\", \"sumpctChgsma10_2\", \n",
    "#     \"sumpctChg+ema30Close_1\", \"pctChgema50\", \"sumpctChgsma5_2\", \"sumpctChg+pctChgema5Close_1\", \"pctChghigh\", \n",
    "#     \"pctChg+sma30Close\", \"pctChgVix\", \"sumpctChgema20_3\", \"sumpctChg+bb_maClose_4\", \"sumpctChg+ema50Close_5\", \n",
    "#     \"sumpctChg+sma20Close_5\", \"pctChg+pctChgsma100Close\", \"sumpctChgema20_2\", \"sumpctChg+pctChgema5Close_5\", \n",
    "#     \"pctChgema30\", \"sumpctChg+sma5Close_6\", \"sumpctChg+ema50Close_3\", \"sumpctChg+pctChgema30Close_1\", \n",
    "#     \"sumpctChgema30_4\", \"sumpctChgema5_3\", 'pctChgclose', 'pctChgema10', 'pctChgema20', 'pctChgema100',\n",
    "# ]\n",
    "features = [\n",
    "    \"sumpctChgvolume_6\", \"sumpctChgbb_high_2\", \"sumpctChgsma50_6\", \n",
    "    \"sumpctChgvolume_5\", \"sumpctChgVix_5\", \"sumpctChgbb_high_3\", \n",
    "    \"sumpctChgvolume_3\", \"sumpctChgVix_3\", \"sumpctChgopen_4\", \n",
    "    \"sumpctChgVix_4\", \"sumpctChghigh_5\", \"sumpctChgsma5_3\", \n",
    "    \"sumpctChgsma50_2\", \"sumpctChgema10_2\", \"sumpctChgbb_ma_4\", \n",
    "    \"pctChgVix\", \"sumpctChgsma20_5\", \"sumpctChgema100_2\", \n",
    "    \"sumpctChglow_1\", \"sumpctChgema30_3\"\n",
    "]\n",
    "# target_features = ['pctChgclose-3','pctChgclose-2','pctChgclose-1','pctChgclose-0','pctChgclose+1','pctChgclose+2','pctChgclose+3','pctChgclose+4','pctChgclose+5','pctChgclose+6']\n",
    "# target_features = ['pctChgclose+1','pctChgclose+2']\n",
    "# target_features = ['sumPctChgclose+1', 'sumPctChgclose+2', 'sumPctChgclose+3', 'sumPctChgclose+4', 'sumPctChgclose+5', 'sumPctChgclose+6']\n",
    "target_features = ['pctChgclose-14', 'pctChgclose-13', 'pctChgclose-12', 'pctChgclose-11', 'pctChgclose-10', 'pctChgclose-9', 'pctChgclose-8', 'pctChgclose-7', 'pctChgclose-6', 'pctChgclose-5', 'pctChgclose-4', 'pctChgclose-3', 'pctChgclose-2', 'pctChgclose-1', 'pctChgclose-0', 'pctChgclose+1', 'pctChgclose+2', 'pctChgclose+3', 'pctChgclose+4', 'pctChgclose+5', 'pctChgclose+6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_modelAE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ajp031/StockDeepLearning/ClusterCast/ClusterCast/HeirarchicalTree.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bacet116-lnx-5.bucknell.edu/home/ajp031/StockDeepLearning/ClusterCast/ClusterCast/HeirarchicalTree.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         initial_weights[layer\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m deepcopy(layer\u001b[39m.\u001b[39mget_weights())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bacet116-lnx-5.bucknell.edu/home/ajp031/StockDeepLearning/ClusterCast/ClusterCast/HeirarchicalTree.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m initial_weights\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bacet116-lnx-5.bucknell.edu/home/ajp031/StockDeepLearning/ClusterCast/ClusterCast/HeirarchicalTree.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m create_modelAE(\u001b[39mlen\u001b[39m(features))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bacet116-lnx-5.bucknell.edu/home/ajp031/StockDeepLearning/ClusterCast/ClusterCast/HeirarchicalTree.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m decoder_weights \u001b[39m=\u001b[39m save_decoder_initial_weights(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_modelAE' is not defined"
     ]
    }
   ],
   "source": [
    "def save_decoder_initial_weights(model):\n",
    "    initial_weights = {}\n",
    "    for layer in model.layers:\n",
    "        if 'input' in layer.name: \n",
    "            continue\n",
    "        print(layer.name)\n",
    "        initial_weights[layer.name] = deepcopy(layer.get_weights())\n",
    "    return initial_weights\n",
    "\n",
    "model = create_modelAE(len(features))\n",
    "decoder_weights = save_decoder_initial_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# x = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]\n",
    "# x = np.array(x)\n",
    "# x = x.reshape(1,1,22)\n",
    "# print(x.shape)\n",
    "# indices = [12, 11, 10, 9, 8, 21, 20, 19, 18, 17, 16, 15, 14, 7, 6, 0, 1, 2, 3, 4, 5]\n",
    "# print(x[:,:,indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "X_train, y_train, X_test, y_test = training_dict['first']\n",
    "X_train_filtered = filter_by_features(X_train, features, feature_dict)\n",
    "X_test_filtered = filter_by_features(X_test, features, feature_dict)\n",
    "y_train_filtered = filter_y_by_features(y_train, target_features, y_feature_dict)\n",
    "y_test_filtered = filter_y_by_features(y_test, target_features, y_feature_dict)\n",
    "print(X_train_filtered.shape)\n",
    "\n",
    "# learning_rates = [.0001,0.001, 0.01, 0.1]\n",
    "# batch_sizes = [10, 20, 50, 100]\n",
    "learning_rates = [.01]\n",
    "batch_sizes = [100]\n",
    "\n",
    "\n",
    "best_score = np.inf\n",
    "best_lr = None\n",
    "best_batch_size = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch in batch_sizes:\n",
    "        # Build and compile the model\n",
    "        test_model = deepcopy(model)\n",
    "        test_model.compile(optimizer=Adam(learning_rate=lr), loss=lambda y_true, y_pred: custom_loss_function(y_true, y_pred, 15, 1.0))\n",
    "\n",
    "        # Callbacks\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"_lr{lr}_batch{batch}_first\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        test_model.fit(X_train_filtered, y_train_filtered, \n",
    "                  validation_data=(X_test_filtered, y_test_filtered),\n",
    "                  epochs=100,  # Adjust as needed\n",
    "                  batch_size=batch,\n",
    "                  callbacks=[tensorboard_callback, early_stopping],\n",
    "                  verbose=1)\n",
    "\n",
    "        # Evaluate the model\n",
    "        score = test_model.evaluate(X_test_filtered, y_test_filtered, verbose=1)\n",
    "\n",
    "        # Update best score\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch\n",
    "            model = deepcopy(test_model)\n",
    "\n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch}, Score: {score}\")\n",
    "\n",
    "# Print best configuration\n",
    "print(f\"Best Score: {best_score}, Learning Rate: {best_lr}, Batch Size: {best_batch_size}\")\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=best_lr), loss='mae')\n",
    "for layer in model.layers:\n",
    "    if 'freeze' in layer.name and '1' in layer.name:\n",
    "        print(layer.name)\n",
    "        layer.trainable = False\n",
    "model.save('path_to_my_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('path_to_my_model2.h5', custom_objects={'custom_loss_function': lambda y_true, y_pred: custom_loss_function(y_true, y_pred, past_steps=15, future_weight=1.0)})\n",
    "for layer in model.layers:\n",
    "    if 'input' in layer.name:\n",
    "        continue\n",
    "    if '1' not in layer.name or 'restore' in layer.name:\n",
    "        print(\"restoring weights for layer {}\".format(layer.name))\n",
    "        layer.set_weights(decoder_weights[layer.name])\n",
    "X_train, y_train, X_test, y_test = training_dict['second']\n",
    "X_train_filtered = filter_by_features(X_train, features, feature_dict)\n",
    "X_test_filtered = filter_by_features(X_test, features, feature_dict)\n",
    "y_train_filtered = filter_y_by_features(y_train, target_features, y_feature_dict)\n",
    "y_test_filtered = filter_y_by_features(y_test, target_features, y_feature_dict)\n",
    "print(X_train_filtered.shape)\n",
    "\n",
    "# learning_rates = [.0001,0.001, 0.01, 0.1]\n",
    "# batch_sizes = [10, 20, 50, 100]\n",
    "learning_rates = [.01]\n",
    "batch_sizes = [20]\n",
    "\n",
    "\n",
    "best_score = np.inf\n",
    "best_lr = None\n",
    "best_batch_size = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch in batch_sizes:\n",
    "        # Build and compile the model\n",
    "        test_model = deepcopy(model)\n",
    "        test_model.compile(optimizer=Adam(learning_rate=lr), loss=lambda y_true, y_pred: custom_loss_function(y_true, y_pred, 15, 1.0))\n",
    "\n",
    "        # Callbacks\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"_lr{lr}_batch{batch}_second\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        test_model.fit(X_train_filtered, y_train_filtered, \n",
    "                  validation_data=(X_test_filtered, y_test_filtered),\n",
    "                  epochs=100,  # Adjust as needed\n",
    "                  batch_size=batch,\n",
    "                  callbacks=[tensorboard_callback, early_stopping],\n",
    "                  verbose=1)\n",
    "\n",
    "        # Evaluate the model\n",
    "        score = test_model.evaluate(X_test_filtered, y_test_filtered, verbose=1)\n",
    "\n",
    "        # Update best score\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch\n",
    "            model = deepcopy(test_model)\n",
    "\n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch}, Score: {score}\")\n",
    "\n",
    "# Print best configuration\n",
    "print(f\"Best Score: {best_score}, Learning Rate: {best_lr}, Batch Size: {best_batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if 'freeze' in layer.name and '2' in layer.name:\n",
    "        print(layer.name)\n",
    "        layer.trainable = False\n",
    "model.compile(optimizer=Adam(learning_rate=best_lr), loss='mae')\n",
    "model.save('path_to_my_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('path_to_my_model2.h5',custom_objects={'rmse': rmse})\n",
    "for layer in model.layers:\n",
    "    if 'input' in layer.name:\n",
    "        continue\n",
    "    if '2' not in layer.name and '1' not in layer.name or 'restore' in layer.name:\n",
    "        layer.set_weights(decoder_weights[layer.name])\n",
    "        print(\"restoring weights for layer {}\".format(layer.name))\n",
    "\n",
    "X_train, y_train, X_test, y_test = training_dict['third']\n",
    "X_train_filtered = filter_by_features(X_train, features, feature_dict)\n",
    "X_test_filtered = filter_by_features(X_test, features, feature_dict)\n",
    "y_train_filtered = filter_y_by_features(y_train, target_features, y_feature_dict)\n",
    "y_test_filtered = filter_y_by_features(y_test, target_features, y_feature_dict)\n",
    "\n",
    "# learning_rates = [.0001,0.001, 0.01, 0.1]\n",
    "# batch_sizes = [10, 20, 50, 100]\n",
    "learning_rates = [.01]\n",
    "batch_sizes = [50]\n",
    "\n",
    "\n",
    "best_score = np.inf\n",
    "best_lr = None\n",
    "best_batch_size = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch in batch_sizes:\n",
    "        # Build and compile the model\n",
    "        test_model = deepcopy(model)\n",
    "        test_model.compile(optimizer=Adam(learning_rate=lr), loss=lambda y_true, y_pred: custom_loss_function(y_true, y_pred, 15, 1.0))\n",
    "\n",
    "        # Callbacks\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"_lr{lr}_batch{batch}_third\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1, restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        test_model.fit(X_train_filtered, y_train_filtered, \n",
    "                  validation_data=(X_test_filtered, y_test_filtered),\n",
    "                  epochs=100,  # Adjust as needed\n",
    "                  batch_size=batch,\n",
    "                  callbacks=[tensorboard_callback, early_stopping],\n",
    "                  verbose=1)\n",
    "\n",
    "        # Evaluate the model\n",
    "        score = test_model.evaluate(X_test_filtered, y_test_filtered, verbose=1)\n",
    "\n",
    "        # Update best score\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch\n",
    "            model = deepcopy(test_model)\n",
    "\n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch}, Score: {score}\")\n",
    "\n",
    "# Print best configuration\n",
    "print(f\"Best Score: {best_score}, Learning Rate: {best_lr}, Batch Size: {best_batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if 'freeze' in layer.name and '3' in layer.name:\n",
    "        print(layer.name)\n",
    "        layer.trainable = False\n",
    "model.compile(optimizer=Adam(learning_rate=best_lr), loss='mae')\n",
    "model.save('path_to_my_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "def eval_model(X_test, y_test_old, model):\n",
    "\n",
    "    predicted_y_old = model.predict(X_test)\n",
    "    predicted_y_old = np.squeeze(predicted_y_old, axis=-1)\n",
    "\n",
    "    print(predicted_y_old.shape)\n",
    "\n",
    "    predicted_y_old = predicted_y_old[:,-6:]\n",
    "    print(predicted_y_old.shape)\n",
    "    y_test_old = y_test_old[:,-6:]\n",
    "    \n",
    "    predicted_y = np.cumsum(predicted_y_old, axis=1)\n",
    "    y_test = np.cumsum(y_test_old, axis=1)\n",
    "   \n",
    "\n",
    "    num_days = predicted_y.shape[1]  # Assuming this is the number of days\n",
    "    print(num_days)\n",
    "    results = pd.DataFrame(predicted_y, columns=[f'predicted_{i+1}' for i in range(num_days)])\n",
    "\n",
    "    for i in range(num_days):\n",
    "        results[f'real_{i+1}'] = y_test[:, i]\n",
    "\n",
    "    # Generate output string with accuracies\n",
    "    output_string = f\"Cluster Number:\\n\"\n",
    "    for i in range(num_days):\n",
    "        results['same_day'] = ((results[f'predicted_{i+1}'] > 0) & (results[f'real_{i+1}'] > 0)) | \\\n",
    "                ((results[f'predicted_{i+1}'] < 0) & (results[f'real_{i+1}'] < 0))\n",
    "        accuracy = round(results['same_day'].mean() * 100,2)\n",
    "\n",
    "        output_string += (\n",
    "            f\"Accuracy{i+1}D {accuracy}% \"\n",
    "            f\"PredictedRet: {results[f'predicted_{i+1}'].mean()} \"\n",
    "            f\"ActRet: {results[f'real_{i+1}'].mean()}\\n\"\n",
    "        )\n",
    "    \n",
    "    output_string += f\"Train set length: {len(X_train_filtered)} Test set length: {len(y_test)}\\n\"\n",
    "\n",
    "    return output_string, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('path_to_my_model2.h5',custom_objects={'rmse': rmse})\n",
    "for layer in model.layers:\n",
    "    if 'restore' in layer.name:\n",
    "        print(\"restoring weights for layer {}\".format(layer.name))\n",
    "        layer.set_weights(decoder_weights[layer.name])\n",
    "\n",
    "X_train, y_train, X_test, y_test = training_dict['last']\n",
    "X_train_filtered = filter_by_features(X_train, features, feature_dict)\n",
    "X_test_filtered = filter_by_features(X_test, features, feature_dict)\n",
    "y_train_filtered = filter_y_by_features(y_train, target_features, y_feature_dict)\n",
    "y_test_filtered = filter_y_by_features(y_test, target_features, y_feature_dict)\n",
    "\n",
    "# learning_rates = [.0001,0.001, 0.01, 0.1]\n",
    "# batch_sizes = [10, 20, 50, 100]\n",
    "learning_rates = [.01]\n",
    "batch_sizes = [10]\n",
    "\n",
    "\n",
    "best_score = np.inf\n",
    "best_lr = None\n",
    "best_batch_size = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch in batch_sizes:\n",
    "        # Build and compile the model\n",
    "        test_model = deepcopy(model)\n",
    "        test_model.compile(optimizer=Adam(learning_rate=lr), loss=lambda y_true, y_pred: custom_loss_function(y_true, y_pred, 15, 1.1))\n",
    "\n",
    "        # Callbacks\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"_lr{lr}_batch{batch}_tuned\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=30, verbose=1, restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        test_model.fit(X_train_filtered, y_train_filtered, \n",
    "                  validation_data=(X_test_filtered, y_test_filtered),\n",
    "                  epochs=250,  # Adjust as needed\n",
    "                  batch_size=batch,\n",
    "                  callbacks=[tensorboard_callback, early_stopping],\n",
    "                  verbose=1)\n",
    "\n",
    "        # Evaluate the model\n",
    "        # score = test_model.evaluate(X_test_filtered, y_test_filtered, verbose=1)\n",
    "\n",
    "        # Update best score\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch\n",
    "            model = deepcopy(test_model)\n",
    "\n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch}, Score: {score}\")\n",
    "\n",
    "# Print best configuration\n",
    "print(f\"Best Score: {best_score}, Learning Rate: {best_lr}, Batch Size: {best_batch_size}\")\n",
    "\n",
    "\n",
    "# val_loss_tuned = model.evaluate(X_test_filtered, y_test_filtered)\n",
    "# print(f'Validation loss: {val_loss_tuned}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fineTunedAccuracy, results_tuned = eval_model(X_test_filtered, y_test_filtered, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model = create_autoencoder(15,len(target_features),len(features))\n",
    "X_train, y_train, X_test, y_test = training_dict['last']\n",
    "X_train_filtered = filter_by_features(X_train, features, feature_dict)\n",
    "X_test_filtered = filter_by_features(X_test, features, feature_dict)\n",
    "y_train_filtered = filter_y_by_features(y_train, target_features, y_feature_dict)\n",
    "y_test_filtered = filter_y_by_features(y_test, target_features, y_feature_dict)\n",
    "\n",
    "print(X_train_filtered.shape)\n",
    "print(y_train_filtered.shape)\n",
    "\n",
    "# learning_rates = [.0001,0.001, 0.01, 0.1]\n",
    "# batch_sizes = [10, 20, 50, 100]\n",
    "\n",
    "learning_rates = [.001]\n",
    "batch_sizes = [16]\n",
    "\n",
    "\n",
    "best_score = np.inf\n",
    "best_lr = None\n",
    "best_batch_size = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch in batch_sizes:\n",
    "        # Build and compile the model\n",
    "        test_model = deepcopy(benchmark_model)\n",
    "        test_model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
    "\n",
    "        # Callbacks\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"_lr{lr}_batch{batch}_bench\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1, restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        test_model.fit(X_train_filtered, y_train_filtered, \n",
    "                  validation_data=(X_test_filtered, y_test_filtered),\n",
    "                  epochs=100,  # Adjust as needed\n",
    "                  batch_size=batch,\n",
    "                  callbacks=[tensorboard_callback, early_stopping],\n",
    "                  verbose=1)\n",
    "\n",
    "        # Evaluate the model\n",
    "        # score = test_model.evaluate(X_test_filtered, y_test_filtered, verbose=1)\n",
    "\n",
    "        # Update best score\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch\n",
    "            benchmark_model = deepcopy(test_model)\n",
    "\n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch}, Score: {score}\")\n",
    "\n",
    "# Print best configuration\n",
    "print(f\"Best Score: {best_score}, Learning Rate: {best_lr}, Batch Size: {best_batch_size}\")\n",
    "\n",
    "# val_loss_benchmark = benchmark_model.evaluate(X_test_filtered, y_test_filtered)\n",
    "\n",
    "benchmarkAccuracy, results_benchmark = eval_model(X_test_filtered, y_test_filtered, benchmark_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "import gc\n",
    "clear_session()\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "def visualize_future_distribution(results):\n",
    "    '''\n",
    "    Create stacked box and whisker plots for the predicted and real values\n",
    "    '''\n",
    "\n",
    "    fig = go.Figure()\n",
    "    print(results.shape)\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        fig.add_trace(go.Box(y=results[f'predicted_{i+1}'], name=f'Predicted {i}')) \n",
    "        fig.add_trace(go.Box(y=results[f'real_{i+1}'], name=f'Real {i}'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Future Performance of Cluster',\n",
    "        xaxis_title='Steps in future',\n",
    "        yaxis_title='Cumulative Percent Change'\n",
    "    ) \n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "bench_fig = visualize_future_distribution(results_benchmark)\n",
    "tuned_fig = visualize_future_distribution(results_tuned)\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "for trace in bench_fig.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "for trace in tuned_fig.data:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(benchmarkAccuracy)\n",
    "# print(val_loss_benchmark)\n",
    "print(fineTunedAccuracy)\n",
    "# print(val_loss_tuned)\n",
    "# write results_tuned dataframe to csv\n",
    "results_tuned.to_csv('results_tuned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
