{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"ClusterCast.settings\"\n",
    "import django\n",
    "django.setup()\n",
    "import sys\n",
    "sys.path.append(\"/home/ajp031/StockDeepLearning/ClusterCast/ClusterCast\")\n",
    "from django.db.models.functions import Now\n",
    "from asgiref.sync import sync_to_async\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, RepeatVector, TimeDistributed, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "import ClusterPipeline.models.RNNModels as rnn\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import ClusterPipeline.models.ClusterProcessing as cp\n",
    "import ClusterPipeline.models.RNNModels as rnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync_to_async\n",
    "def get_all_objects(features,steps = None):\n",
    "    # Force the query to execute and load all results into memory\n",
    "    params = list(cp.StockClusterGroupParams.objects.all())\n",
    "    matching_groups = []\n",
    "    for param in params:\n",
    "        if steps is None:\n",
    "            if param.cluster_features == features:\n",
    "                group = cp.StockClusterGroup.objects.get(pk=param.pk)\n",
    "                matching_groups.append(group)\n",
    "                group.load_saved_group()\n",
    "\n",
    "        else: \n",
    "            if param.cluster_features == features and param.n_steps == steps:\n",
    "                group = cp.StockClusterGroup.objects.get(pk=param.pk)\n",
    "                matching_groups.append(group)\n",
    "                group.load_saved_group()\n",
    "                print(group.group_params.tickers)\n",
    "    \n",
    "    return matching_groups\n",
    "\n",
    "\n",
    "cluster_features = [\"close\", \"bb_low\", \"bb_high\"]\n",
    "steps = 20\n",
    "\n",
    "async def create_cluster_group_params(cluster_features, steps=20):\n",
    "\n",
    "    cluster_groups = await get_all_objects(features=cluster_features,steps=steps)\n",
    "    return cluster_groups\n",
    "\n",
    "# Run the async function\n",
    "\n",
    "matching_groups = await create_cluster_group_params(cluster_features, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync_to_async\n",
    "def get_all_clusters(groups):\n",
    "    clusters = []\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        clusters += group.clusters\n",
    "\n",
    "    for cluster in clusters:\n",
    "        models = rnn.RNNModel.objects.filter(cluster=cluster)\n",
    "        for model in models: \n",
    "            if model:\n",
    "                features += (model.model_features)\n",
    "\n",
    "    features = list(set(features))\n",
    "    return clusters, features\n",
    "\n",
    "clusters, all_features = await get_all_clusters(matching_groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cluster = clusters[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "features = random.sample(all_features, 50)\n",
    "\n",
    "feature_indices = [test_cluster.X_feature_dict[feature] for feature in features] \n",
    "test_cluster.X_train_filtered = test_cluster.X_train[:,:,feature_indices]\n",
    "test_cluster.X_test_filtered = test_cluster.X_test[:,:,feature_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dropout, TimeDistributed, Dense, Concatenate, Permute, Reshape, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Activation, Flatten\n",
    "# Custom Attention Layer\n",
    "\n",
    "def create_model_with_attention2(input_shape, latent_dim=6):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(None, input_shape))\n",
    "\n",
    "    # Encoder\n",
    "\n",
    "    encoder_lstm1 = LSTM(units=50, activation='tanh', return_sequences=True, name='encoder_lstm_1_restore')(input_layer)\n",
    "    encoder_dropout1 = Dropout(0.2, name='encoder_dropout_1_restore')(encoder_lstm1)\n",
    "\n",
    "    encoder_lstm2 = LSTM(units=25, activation='tanh', return_sequences=True, name='encoder_lstm_2_restore')(encoder_dropout1)\n",
    "    encoder_dropout2 = Dropout(0.2, name='encoder_dropout_2_restore')(encoder_lstm2)\n",
    "\n",
    "    encoder_lstm3 = LSTM(units=10, activation='tanh', return_sequences=True, name='encoder_lstm_3_restore')(encoder_dropout2)\n",
    "    encoder_dropout3 = Dropout(0.2, name='encoder_dropout_3_restore')(encoder_lstm3)\n",
    "\n",
    "    output_lstm = LSTM(units=6, activation='tanh', return_sequences=True, name='outputLSTM')(encoder_dropout3)\n",
    "    encoder_output = Dropout(0.2, name='encoder_output')(output_lstm)\n",
    "\n",
    "    # encoder_lstm4 = LSTM(units=50, activation='tanh', return_sequences=True, name='encoder_lstm_4_restore')(encoder_dropout3)\n",
    "    # encoder_dropout4 = Dropout(0.2, name='encoder_dropout_4_restore')(encoder_lstm4)\n",
    "\n",
    "    # Attention Layer\n",
    "    # attention = AttentionLayer(name='attention_layer')(encoder_dropout4)\n",
    "     # Attention Mechanism\n",
    "    attention = Dense(1, activation='tanh')(encoder_output)\n",
    "    attention = Flatten()(attention)\n",
    "    attention_weights = Activation('softmax')(attention)\n",
    "    context = Multiply()([encoder_output, Permute([2, 1])(RepeatVector(6)(attention_weights))])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_lstm1 = LSTM(units=75, activation='tanh', return_sequences=True, name='decoder_lstm_1_restore')(context)\n",
    "    decoder_dropout1 = Dropout(0.2, name='decoder_dropout_1_restore')(decoder_lstm1)\n",
    "\n",
    "    decoder_lstm2 = LSTM(units=25, activation='tanh', return_sequences=True, name='decoder_lstm_2_restore')(decoder_dropout1)\n",
    "    decoder_dropout2 = Dropout(0.2, name='decoder_dropout_2_restore')(decoder_lstm2)\n",
    "\n",
    "    decoder_lstm3 = LSTM(units=10, activation='tanh', return_sequences=True, name='decoder_lstm_3_restore')(decoder_dropout2)\n",
    "    decoder_dropout3 = Dropout(0.2, name='decoder_dropout_3_restore')(decoder_lstm3)\n",
    "\n",
    "    time_distributed_output = TimeDistributed(Dense(1), name='time_distributed_output')(decoder_dropout3)\n",
    "\n",
    "    final_output = time_distributed_output[:, -6:, :]\n",
    "\n",
    "    # Create the model\n",
    "    model_lstm = Model(inputs=input_layer, outputs=final_output)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model_lstm.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    return model_lstm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 23:07:37.982469: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-10 23:07:39.452525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, Permute, Multiply, RepeatVector, Lambda, TimeDistributed, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def attention_mechanism(encoder_out_seq, decoder_out_seq,encoder_hidden_units):\n",
    "    # encoder_out_seq: (batch_size, time_steps, encoder_hidden_units)\n",
    "    # decoder_out_seq: (batch_size, decoder_hidden_units)\n",
    "    \n",
    "    # Calculating attention scores\n",
    "    score = Dense(encoder_hidden_units)(decoder_out_seq)\n",
    "    score = Lambda(lambda x: K.batch_dot(*x, axes=[2, 2]))([score, encoder_out_seq])\n",
    "\n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = Activation('softmax')(score)\n",
    "\n",
    "    # Calculate context vector as weighted sum of encoder outputs\n",
    "    context_vector = Lambda(lambda x: K.batch_dot(*x, axes=[1, 1]))([attention_weights, encoder_out_seq])\n",
    "    \n",
    "    return context_vector\n",
    "\n",
    "def create_model_with_custom_attention(input_shape, output_steps, latent_dim=6):\n",
    "    # Encoder\n",
    "    input_layer = Input(shape=(None, input_shape))\n",
    "\n",
    "    encoder_lstm1 = LSTM(units=50, activation='tanh', return_sequences=True, name='encoder_lstm_1_restore')(input_layer)\n",
    "    encoder_dropout1 = Dropout(0.2, name='encoder_dropout_1_restore')(encoder_lstm1)\n",
    "\n",
    "    encoder_lstm2 = LSTM(units=25, activation='tanh', return_sequences=True, name='encoder_lstm_2_restore')(encoder_dropout1)\n",
    "    encoder_dropout2 = Dropout(0.2, name='encoder_dropout_2_restore')(encoder_lstm2)\n",
    "\n",
    "    encoder_lstm3 = LSTM(units=10, activation='tanh', return_sequences=True, name='encoder_lstm_3_restore')(encoder_dropout2)\n",
    "    encoder_dropout3 = Dropout(0.2, name='encoder_dropout_3_restore')(encoder_lstm3)\n",
    "\n",
    "    output_lstm = LSTM(units=6, activation='tanh', return_sequences=True, name='outputLSTM')(encoder_dropout3)\n",
    "\n",
    "    encoder_output = Dropout(0.2, name='encoder_output')(output_lstm)\n",
    "\n",
    "    # Prepare initial state for decoder (optional, depending on your design)\n",
    "    decoder_initial_state = Dense(latent_dim)(encoder_output[:,-1,:])\n",
    "\n",
    "    # Decoder\n",
    "    all_outputs = []\n",
    "    decoder_input = RepeatVector(1)(decoder_initial_state)  # initial input for decoder\n",
    "    for _ in range(output_steps):\n",
    "        # Attention mechanism\n",
    "        context_vector = attention_mechanism(encoder_output, decoder_input,6)\n",
    "        \n",
    "        # Combine context vector with previous decoder output\n",
    "        decoder_lstm_input = Concatenate(axis=-1)([context_vector, decoder_input])\n",
    "        \n",
    "        # Decoder LSTM followed by dropout\n",
    "        decoder_output = LSTM(latent_dim, return_sequences=True, return_state=True)(decoder_lstm_input)\n",
    "        decoder_output, _ = Dropout(0.2)(decoder_output)\n",
    "        \n",
    "        # TimeDistributed Dense layer to make the prediction\n",
    "        decoder_output = TimeDistributed(Dense(1))(decoder_output)\n",
    "        all_outputs.append(decoder_output)\n",
    "\n",
    "        # Update decoder_input with the output for next time step\n",
    "        decoder_input = decoder_output\n",
    "\n",
    "    # Concatenate all predicted outputs\n",
    "    final_output = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=input_layer, outputs=final_output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, latent_dim=6):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(None, input_shape))\n",
    "\n",
    "    # masking_layer = Masking(mask_value=0.0, name='masking_layer')(input_layer)\n",
    "\n",
    "    # Encoder\n",
    "\n",
    "    encoder_lstm2 = LSTM(units=50, activation='tanh', return_sequences=True, name='encoder_lstm_2_restore')(input_layer)\n",
    "    encoder_dropout2 = Dropout(0.2, name='encoder_dropout_2_restore')(encoder_lstm2)\n",
    "\n",
    "    encoder_lstm3 = LSTM(units=100, activation='tanh', return_sequences=False, name='encoder_lstm_3_restore')(encoder_dropout2)\n",
    "    encoder_dropout3 = Dropout(0.2, name='encoder_dropout_3_restore')(encoder_lstm3)\n",
    "\n",
    "    # encoder_lstm4 = LSTM(units=50, activation='tanh', return_sequences=False, name='encoder_lstm_4_restore')(encoder_dropout3)\n",
    "    # encoder_dropout4 = Dropout(0.2, name='encoder_dropout_4_restore')(encoder_lstm4)\n",
    "\n",
    "    # Repeat Vector\n",
    "    repeat_vector = RepeatVector(latent_dim, name='repeat_vector')(encoder_dropout3)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_lstm1 = LSTM(units=50, activation='tanh', return_sequences=True, name='decoder_lstm_1_restore')(repeat_vector)\n",
    "    decoder_dropout1 = Dropout(0.2, name='decoder_dropout_1_restore')(decoder_lstm1)\n",
    "\n",
    "    decoder_lstm2 = LSTM(units=25, activation='tanh', return_sequences=True, name='decoder_lstm_2_restore')(decoder_dropout1)\n",
    "    decoder_dropout2 = Dropout(0.2, name='decoder_dropout_2_restore')(decoder_lstm2)\n",
    "\n",
    "    time_distributed_output = TimeDistributed(Dense(1), name='time_distributed_output')(decoder_dropout2)\n",
    "\n",
    "\n",
    "    # Create the model\n",
    "    model_lstm = Model(inputs=input_layer, outputs=time_distributed_output)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model_lstm.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    return model_lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = test_cluster.X_train_filtered\n",
    "y_train = test_cluster.y_train\n",
    "X_test = test_cluster.X_test_filtered\n",
    "y_test = test_cluster.y_test\n",
    "\n",
    "start_token = 0\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_model = create_model(X_train.shape[2], latent_dim=6)\n",
    "attention_model = create_model_with_attention2(X_train.shape[2], latent_dim=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "def eval_model(X_test, y_test, model,cluster):\n",
    "\n",
    "    predicted_y = model.predict(X_test)\n",
    "    predicted_y = np.squeeze(predicted_y, axis=-1)\n",
    "\n",
    "    num_days = predicted_y.shape[1]  # Assuming this is the number of days\n",
    "    results = pd.DataFrame(predicted_y, columns=[f'predicted_{i+1}' for i in range(num_days)])\n",
    "\n",
    "    for i in range(num_days):\n",
    "        results[f'real_{i+1}'] = y_test[:, i]\n",
    "\n",
    "    # Generate output string with accuracies\n",
    "    output_string = f\"Cluster Number: {cluster.label}\\n\"\n",
    "    for i in range(num_days):\n",
    "        same_day = ((results[f'predicted_{i+1}'] > 0) & (results[f'real_{i+1}'] > 0)) | \\\n",
    "                ((results[f'predicted_{i+1}'] < 0) & (results[f'real_{i+1}'] < 0))\n",
    "        accuracy = round(same_day.mean() * 100,2)\n",
    "\n",
    "        output_string += (\n",
    "            f\"Accuracy{i+1}D {accuracy}% \"\n",
    "            f\"PredictedRet: {results[f'predicted_{i+1}'].mean()} \"\n",
    "            f\"ActRet: {results[f'real_{i+1}'].mean()}\\n\"\n",
    "        )\n",
    "    \n",
    "    output_string += f\"Train set length: {len(X_train)} Test set length: {len(y_test)}\\n\"\n",
    "\n",
    "    return output_string, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "regular_model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Set up early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=35, restore_best_weights=True)\n",
    "\n",
    "# Fine-tune the model using your smaller dataset\n",
    "# Assume you have 'small_train_data', 'small_train_labels', 'small_val_data', and 'small_val_labels'\n",
    "history = regular_model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=250,\n",
    "                    batch_size=16,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance\n",
    "val_loss = regular_model.evaluate(X_test, y_test)\n",
    "regularAccuracy,regularResults = eval_model(X_test, y_test, regular_model,test_cluster)\n",
    "clear_session()\n",
    "del regular_model\n",
    "print(f'Validation loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "attention_model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Set up early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=35, restore_best_weights=True)\n",
    "\n",
    "# Fine-tune the model using your smaller dataset\n",
    "# Assume you have 'small_train_data', 'small_train_labels', 'small_val_data', and 'small_val_labels'\n",
    "history = attention_model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=250,\n",
    "                    batch_size=16,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance\n",
    "val_loss = attention_model.evaluate(X_test, y_test)\n",
    "print(f'Validation loss: {val_loss}')\n",
    "attentionAccuracy,attentionResults = eval_model(X_test, y_test, attention_model,test_cluster)\n",
    "clear_session()\n",
    "del attention_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "def visualize_future_distribution(results):\n",
    "    '''\n",
    "    Create stacked box and whisker plots for the predicted and real values\n",
    "    '''\n",
    "\n",
    "    fig = go.Figure()\n",
    "    print(results.shape)\n",
    "\n",
    "    for i in range(6):\n",
    "\n",
    "        fig.add_trace(go.Box(y=results[f'predicted_{i+1}'], name=f'Predicted {i}')) \n",
    "        fig.add_trace(go.Box(y=results[f'real_{i+1}'], name=f'Real {i}'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Future Performance of Cluster',\n",
    "        xaxis_title='Steps in future',\n",
    "        yaxis_title='Cumulative Percent Change'\n",
    "    ) \n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "bench_fig = visualize_future_distribution(regularResults)\n",
    "tuned_fig = visualize_future_distribution(attentionResults)\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "for trace in bench_fig.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "for trace in tuned_fig.data:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regularAccuracy)\n",
    "print(attentionAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
